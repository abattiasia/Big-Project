{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49a967-c9f2-4451-bcba-f9517fa9dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 6: Big Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d710a9cb-73ca-4079-803e-b4415d024b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class tasks():\n",
    "    def __init__(self):\n",
    "        print(f\"\"\"\n",
    "hier sind paar tasks 01 bis 07\n",
    "you can make object from each Task\n",
    "a= tasks()\n",
    "help, run =a.task01()\n",
    "help(0)  # only see what is the task\n",
    "#run(0)  # if you want to run Task\n",
    "\"\"\")\n",
    "        \n",
    "\n",
    "    def task01(self):\n",
    "        def help(self):\n",
    "            print (f\"\"\"\n",
    "    Task 1: Build a Financial Analyst System Using LangChain and Ollama\n",
    "    Requirements:\n",
    "    Use LangChain and Ollama to create a financial analyst system.\n",
    "    The system should accept user queries related to finance (e.g., stock analysis, market trends, company reports).\n",
    "    It should be able to analyze, process, and provide useful insights or answers from financial data.\n",
    "1\n",
    "    \"\"\")\n",
    "            \n",
    "        def run(self):\n",
    "            ### Chat Streaming \n",
    "            from llama_index.llms.ollama import Ollama\n",
    "            from llama_index.core.llms import ChatMessage\n",
    "            \n",
    "            # Initialize the model\n",
    "            llm = Ollama(model=\"llama3.2\")\n",
    "            \n",
    "            messages = [\n",
    "                ChatMessage(\n",
    "                    role=\"system\", content=\"You are a financial analyst system\"\n",
    "                ),\n",
    "                ChatMessage(role=\"user\", content=\"Write reort about financial services\"),\n",
    "            ]\n",
    "            \n",
    "            response = llm.stream_chat(messages)\n",
    "            for r in response:\n",
    "                print(r.delta, end=\"\")\n",
    "        return  help, run\n",
    "\n",
    "    ##########################################################################\n",
    "            \n",
    "\n",
    "    def task02(self):\n",
    "        def help(self):\n",
    "            print(f\"\"\"\n",
    "    Task 2: Image Classification Using Transfer Learning and a Built-in Dataset\n",
    "    Requirements:\n",
    "    Use any transfer learning model (e.g., VGG16, ResNet, or MobileNet) to build a neural network for image classification.\n",
    "    Select a built-in dataset for image classification (e.g., CIFAR-10, MNIST, or Fashion MNIST).\n",
    "    Preprocess the dataset and ensure itâ€™s ready for input into the model.\n",
    "    Fine-tune the pre-trained model by replacing the final layers to suit the classification task.\n",
    "    Train and evaluate the model on the dataset, reporting accuracy and loss.\n",
    "\n",
    "    \"\"\")\n",
    "        def run(self):\n",
    "            import keras\n",
    "            from keras.datasets import fashion_mnist\n",
    "            from keras.models import Sequential\n",
    "            from keras.layers import Dense, Flatten\n",
    "            from keras.utils import to_categorical\n",
    "\n",
    "            keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "            # 1. Load the .fashion_mnist Dataset\n",
    "            (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "            # 2. Preprocess the Data\n",
    "            # Normalize the data by scaling pixel values to be between 0 and 1\n",
    "            X_train = X_train.astype('float32') / 255\n",
    "            X_test = X_test.astype('float32') / 255\n",
    "\n",
    "            # Initialize the Sequential model\n",
    "            from tensorflow.keras import layers\n",
    "            model_cnn = Sequential()  #models.\n",
    "\n",
    "            # Add layers step-by-step using the add() method\n",
    "            model_cnn.add(layers.InputLayer(input_shape=(28, 28, 1)))  # Input layer for 32x32x3 color images\n",
    "            model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu'))  # First Conv2D layer\n",
    "            model_cnn.add(layers.MaxPooling2D((2, 2)))  # First MaxPooling layer\n",
    "\n",
    "            model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu'))  # Second Conv2D layer\n",
    "            model_cnn.add(layers.MaxPooling2D((2, 2)))  # Second MaxPooling layer\n",
    "\n",
    "            model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu'))  # Third Conv2D layer\n",
    "\n",
    "            model_cnn.add(layers.Flatten())  # Flatten the feature maps\n",
    "            model_cnn.add(layers.Dense(64, activation='relu'))  # Fully connected layer\n",
    "            model_cnn.add(layers.Dense(10, activation='softmax'))  # Output layer for 10 classes\n",
    "\n",
    "\n",
    "            # Compile the model\n",
    "            model_cnn.compile(optimizer='adam',\n",
    "                          loss='sparse_categorical_crossentropy',   #sparse_categorical_crossentropy  ..> only wenn no encoding for y_train\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            history = model_cnn.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "            # Evaluate the model\n",
    "            test_loss, test_acc = model_cnn.evaluate(X_test, y_test)\n",
    "            test_acc\n",
    "\n",
    "\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            # Display the first test image\n",
    "            plt.imshow(X_test[0]) #, cmap='color')\n",
    "            plt.title(\"First Test Image\")\n",
    "            plt.show()\n",
    "\n",
    "            # Plot training and validation accuracy/loss\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            class_names = ['Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "            plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history.history['loss'], label='Training Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.show()\n",
    "            # Display the first 5 test images and predicted labels\n",
    "            predictions = model_cnn.predict(X_test[:10])\n",
    "\n",
    "\n",
    "            # Plot training and validation accuracy/loss\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            class_names = ['Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "            plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history.history['loss'], label='Training Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.show()\n",
    "            # Display the first 5 test images and predicted labels\n",
    "            predictions = model_cnn.predict(X_test[:10])\n",
    "\n",
    "            plt.figure(figsize=(10, 2))\n",
    "            for i in range(5):\n",
    "                plt.subplot(1, 5, i+1)\n",
    "                plt.imshow(X_test[i])\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                #plt.title(f\"Pred: {class_names[predictions[i].argmax()]}\")\n",
    "            plt.show()\n",
    "        return  help, run\n",
    "    #########################################################################################\n",
    "            \n",
    "    def task03(self):\n",
    "        def help(self):\n",
    "            print(f\"\"\"\n",
    "    Task 3:Convert Sentences to Embeddings Using Ollama and Implement Similarity Search with a Vector Database\n",
    "    Requirements:\n",
    "    Use Ollama to convert three sample sentences into embeddings.\n",
    "    Store the embeddings in a vector database (e.g., FAISS or Pinecone).\n",
    "    Implement a similarity search function to compare the embeddings and return the most similar sentence for a given query.\n",
    "    Test the system by inputting a new sentence and retrieving the most semantically similar one from the stored sentences.\n",
    "\n",
    "    \"\"\")\n",
    "            \n",
    "        def run(self):\n",
    "                        \n",
    "            # generate embeddings \n",
    "            from sentence_transformers import SentenceTransformer\n",
    "\n",
    "            # Load a pre-trained sentence transformer model\n",
    "            model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "            # # Example words to embed\n",
    "            # words = [\"apple\", \"banana\", \"car\", \"train\"]\n",
    "\n",
    "            # Sample sentences to embed\n",
    "            texts = [ \"LangChain is a framework for building applications using LLMs.\", \n",
    "            \"Ollama provides easy access to various AI models.\", \n",
    "            \"Generative AI is revolutionizing industries.\" ]\n",
    "\n",
    "\n",
    "            # Generate embeddings\n",
    "            embeddings = model.encode(texts)\n",
    "\n",
    "\n",
    "            # Print the embeddings\n",
    "            for text, embedding in zip(texts, embeddings):\n",
    "                print(f\"Word: {text}, Embedding: {embedding}\")\n",
    "\n",
    "            from qdrant_client import QdrantClient\n",
    "            client = QdrantClient(url=\"http://localhost:6333\")\n",
    "            from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "            #Create a new collection for the words\n",
    "            collection_name = \"Pinecone\"\n",
    "            client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                vectors_config=VectorParams(size=embeddings.shape[1], distance=Distance.DOT)  # Use the size of the embedding\n",
    "            )\n",
    "\n",
    "            from qdrant_client.http.models import PointStruct\n",
    "            points = [\n",
    "                PointStruct(\n",
    "                    id=i + 1,  # Assign a unique ID to each point\n",
    "                    vector=embedding.tolist()  # Convert embedding to a list for insertion\n",
    "                )\n",
    "                for i, embedding in enumerate(embeddings)  # Loop through embeddings and create PointStruct\n",
    "            ]\n",
    "\n",
    "            client.upsert(\n",
    "                collection_name=collection_name,\n",
    "                points=points  # Insert all points into the collection\n",
    "            )\n",
    "        return  help, run\n",
    "    ##############################################################################################\n",
    "            \n",
    "\n",
    "    def task04(self):\n",
    "        \n",
    "        def help(self):\n",
    "            print(f\"\"\"\n",
    "    Task 4 : Build a Sentiment Analysis System Using LangChain and Ollama\n",
    "    Requirements:\n",
    "    Use LangChain integrated with Ollama to build a sentiment analysis system.\n",
    "    The system should classify text inputs (e.g., product reviews, social media posts) into positive, negative, or neutral sentiment.\n",
    "    Use a pre-trained model (ollama) to handle text embedding and sentiment classification.\n",
    "\n",
    "    \"\"\")\n",
    "            \n",
    "        def run(self):\n",
    "            from langchain_ollama.llms import OllamaLLM\n",
    "            from langchain.prompts import PromptTemplate\n",
    "            from langchain.chains import LLMChain\n",
    "\n",
    "            # 01 Initialize Ollama model\n",
    "            ollama = OllamaLLM(model=\"llama3.2\") \n",
    "\n",
    "            # 02 Define a LangChain prompt template for sentiment analysis\n",
    "            template = \"\"\"\n",
    "            Classify input text into: positive, negative, or neutral Text: '{text}'\"\n",
    "            \"\"\"\n",
    "            prompt = PromptTemplate(input_variables=[\"text\"], template=template)\n",
    "\n",
    "            # 03 Creating an LLM chain with Ollama and the prompt template\n",
    "            chain = LLMChain(llm=ollama, prompt=prompt)\n",
    "\n",
    "            # 04 using model to handle text embedding and sentiment classification\n",
    "            input_text = \"good product! It works very good\"  #\"bad product! It dosnt work \"\n",
    "            result = chain.run(input_text)\n",
    "            print(f\"Sentiment: {result}\")\n",
    "        return  help, run\n",
    "\n",
    "    ########################################################################\n",
    "            \n",
    "    def task05(self):\n",
    "        def help(self):\n",
    "            print(f\"\"\"\n",
    "\n",
    "    \"\"\")\n",
    "            \n",
    "        def run(self):\n",
    "                    \n",
    "            from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "            llm = OllamaLLM(model=\"llama3.2\",model_kwargs={\"temperature\": 1, \"max_tokens\": 200})\n",
    "            while True:\n",
    "                prompt = input(\"Ask your question, or type 'exit' \")\n",
    "                if prompt.lower() == \"exit\":\n",
    "                    print(\"Exit\")\n",
    "                    break\n",
    "                answer = llm(prompt)\n",
    "                print(answer)\n",
    "\n",
    "            from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "            # 01 Initialize the Ollama LLM\n",
    "            llm = OllamaLLM(model=\"llama3.2\")  #model=\"llama3.2\", model=\"deepseek-coder-v2\", model=\"qwen2\"\n",
    "\n",
    "            # Make a request to the model\n",
    "            response = llm(\"please find the 10 germany movies in historz, return data as json\")\n",
    "            print(response)\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            # Task 5 part 2\n",
    "            from llama_index.llms.ollama import Ollama\n",
    "            from llama_index.core.llms import ChatMessage\n",
    "\n",
    "            llm = Ollama(model=\"llama3.2\",model_kwargs={\"temperature\": 1, \"max_tokens\": 200})\n",
    "            messages = [ ChatMessage(role=\"user\", content= prompt)]\n",
    "            while True:\n",
    "                prompt = input(\"Ask your question, or type 'exit' \")\n",
    "                if prompt.lower() == \"exit\":\n",
    "                    print(\"Exit\")\n",
    "                    break\n",
    "                answer = llm.chat(messages)\n",
    "                print(answer)\n",
    "\n",
    "            from llama_index.llms.ollama import Ollama\n",
    "            from llama_index.core.llms import ChatMessage\n",
    "\n",
    "            # Initialize the LLM\n",
    "            llm = Ollama(model=\"llama3.2\", model_kwargs={\"temperature\": 1, \"max_tokens\": 200})\n",
    "\n",
    "            while True:\n",
    "                prompt = input(\"Ask your question, or type 'exit': \")\n",
    "                if prompt.lower() == \"exit\":\n",
    "                    print(\"Exit\")\n",
    "                    break\n",
    "                \n",
    "                # Add the user message to the messages list\n",
    "                messages = [(ChatMessage(role=\"user\", content=prompt))]\n",
    "                \n",
    "                # Get the answer from the LLM\n",
    "                answer = llm.chat(messages)\n",
    "                print(answer)\n",
    "            \n",
    "\n",
    "            ### Chat Streaming \n",
    "            from llama_index.llms.ollama import Ollama\n",
    "            from llama_index.core.llms import ChatMessage\n",
    "\n",
    "            # Initialize the model\n",
    "            llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "            messages = [\n",
    "                ChatMessage(\n",
    "                    role=\"system\", content=\"You are helpful assistant to create programs\"\n",
    "                ),\n",
    "                ChatMessage(role=\"user\", content=\"Write a python program to calculate the fact of numbers\"),\n",
    "            ]\n",
    "\n",
    "            response = llm.stream_chat(messages)\n",
    "            for r in response:\n",
    "                print(r.delta, end=\"\")\n",
    "                \n",
    "\n",
    "            from llama_index.llms.ollama import Ollama\n",
    "            from llama_index.core.llms import ChatMessage\n",
    "\n",
    "            # Initialize the model\n",
    "            llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "            messages = [\n",
    "             #   ChatMessage(\n",
    "              #      role=\"system\", content=\"You are helpful assistant to create programs\"\n",
    "             #   ),\n",
    "                ChatMessage(role=\"user\", content=\"please find the 10 germany movies from 2000, return data as json\"),\n",
    "            ]\n",
    "\n",
    "            response = llm.chat(messages)\n",
    "            print(response)\n",
    "        return  help, run\n",
    "            \n",
    "    ################################################################################\n",
    "     \n",
    "    def task06(self):\n",
    "        def help(self):\n",
    "            print(f\"\"\"\n",
    "    Task 6 : Big Project\n",
    "    Requirements:\n",
    "    integrate the last 5 projects in one main class , each project in a function inside this class and add a main function which starts asking the user which app he wants to run if the user press the app numbers (like the game we did before)\n",
    "\n",
    "    \"\"\")\n",
    "        def run(self):\n",
    "            print(f\" this Task is don\")\n",
    "        return  help, run\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "            \n",
    "    def task07(self):\n",
    "        def help(self):\n",
    "            print(f\"\"\"\n",
    "    Task 7 Text Summarization using LangChain and Ollama\n",
    "    Requirements: \n",
    "    The goal is to use LangChain and the Ollama to build a python script that takes a block of text and summarizes it into a concise version. \n",
    "    Build the system again using llamaindex \n",
    "\n",
    "    \"\"\")\n",
    "            \n",
    "        def run(self):\n",
    "                    \n",
    "            #!pip install upgrade langchain\n",
    "\n",
    "            from langchain.llms import Ollama\n",
    "            from langchain.prompts import PromptTemplate\n",
    "            from langchain.chains import LLMChain\n",
    "            from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "            # 01 Initialize the Ollama LLM\n",
    "            llm = OllamaLLM(model=\"llama3.2\")  #model=\"llama3.2\", model=\"deepseek-coder-v2\", model=\"qwen2\"\n",
    "\n",
    "            # Define the prompt template for summarization\n",
    "            prompt_template = PromptTemplate(\n",
    "                input_variables=[\"text\"],\n",
    "                template=\"Please summarize the following text:\\n\\n{text}\\n\\nSummary:\"\n",
    "            )\n",
    "\n",
    "            # Create an LLMChain for summarization\n",
    "            llm_chain = LLMChain(llm=model, prompt=prompt_template)\n",
    "\n",
    "            text_to_summarize = \"\"\" Generative AI refers to a category of artificial intelligence that focuses on creating new content based on learned patterns \n",
    "            from existing data. Unlike traditional AI, which typically analyzes data to perform specific tasks, generative AI goes a step further by generating \n",
    "            original outputs, such as text, images, music, and even video. Machine Learning Models: Generative AI relies heavily on machine learning techniques, \n",
    "            particularly deep learning. Models like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) are commonly used to produce new \n",
    "            data by understanding the underlying distributions of the input data. Applications: The applications of generative AI are vast and growing. \n",
    "            In creative fields, it can be used to generate art, music, and literature. In the business sector, it helps in creating realistic simulations \n",
    "            for training purposes or generating marketing content. Additionally, it plays a significant role in game development by producing unique characters \n",
    "            and environments. Text Generation: One of the most prominent areas of generative AI is text generation. Models like GPT (Generative Pre-trained \n",
    "            Transformer) are trained on large corpuses of text and can produce human-like responses, write essays, summarize information, and even code. \n",
    "            These models have been used in chatbots, virtual assistants, and content creation tools. Ethical Considerations: The rise of generative AI \n",
    "            also raises ethical questions. Concerns about copyright, misinformation, and the potential for misuse (such as generating deepfakes) have led \n",
    "            to discussions about the responsible use of this technology. Ensuring transparency and accountability in generative AI systems is critical to \n",
    "            addressing these issues. Future Directions: As generative AI continues to evolve, we can expect advancements that improve the quality and diversity\n",
    "            of generated content. The integration of generative AI with other technologies, such as augmented reality (AR) and virtual reality (VR), may lead\n",
    "            to immersive experiences that blur the lines between reality and creation. In summary, generative AI represents a significant advancement\n",
    "            in the field of artificial intelligence, enabling machines to create original content and push the boundaries of creativity and innovation. \n",
    "            Its impact is felt across multiple industries, prompting ongoing discussions about its ethical use and future potential.\n",
    "            \"\"\"\n",
    "            # Get the summary\n",
    "            summary = llm_chain.run(text_to_summarize)\n",
    "            print(\"Original Text:\\n\", text_to_summarize)\n",
    "            print(\"\\nSummary:\\n\", summary)\n",
    "\n",
    "            #!pip install llama-index upgrade\n",
    "        return  help, run\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
